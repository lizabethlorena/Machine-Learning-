{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mT7Mn_pSQVyc"
   },
   "source": [
    "# Machine Learning- Final Exam\n",
    "# Name: Lizabeth Singh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "z_XyW0s7QVyg"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import pprint as pp\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import scipy as sp\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.linear_model import Lasso\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns; sns.set()  # for plot styling\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import scale\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import scale\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.cluster import hierarchy\n",
    "from scipy.cluster.hierarchy import linkage \n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "from scipy.spatial.distance import pdist, squareform\n",
    "from scipy.cluster.hierarchy import dendrogram\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Activation\n",
    "from tensorflow.keras.layers import Dense\n",
    "import tensorflow.keras as keras\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, Activation\n",
    "from tensorflow.keras.optimizers import SGD\n",
    "from tensorflow.keras.models import Sequential\n",
    "from keras import backend as K  \n",
    "from keras.models import Sequential\n",
    "from keras.layers.core import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers.convolutional import Convolution2D, MaxPooling2D\n",
    "from keras.utils import np_utils\n",
    "from keras.layers.convolutional import Conv2D\n",
    "\n",
    "\n",
    "#Here I am importing packages that will be essential in my analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uVZGJDuLQVyh"
   },
   "source": [
    "# 1.  From the perspective of a social scientist, which models did we learn this semester that are useful for ruling out alternative explanations through control variables AND that allow us to observe substantively meaningful information from model coefficients?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aHw5V49SQVyi"
   },
   "source": [
    "In this class, we have learned that there is a difference between classification models and regression models. For this promp, we are soley looking at regression models and what is best to use for various instances. For a regression we are seeing how well x variables explain the y variable. These models entails linear, ridge, lasso, random forest, bagged trees, decision tree, SVM, and KNN. Although these models can be used to regression analysis, some can also be used for classicication models and prediction. These models include KNN, logistic regression, penalized logistic regression, SVM, decision tree, bagged trees, and random forest. When using these models we take into consideration not only what they are meant to be used for but also if scaling is required, whether or whether not continous exponents or varibales only, wheather or whather not tuning paramters is necessary, if it can hold a large file size,and how fast or slow the prediction process takes to run."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "flpVawVZQVyi"
   },
   "source": [
    "# 2. Describe the main differences between supervised and unsupervised learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IV1RqpKvQVyj"
   },
   "source": [
    "The main difference between supervised and unsupervised learning is labeled datasets. Labeled data is used to train the data/ algorithms for either classification or prediction outcomes. By executing this the model can view its accuracy for instance. Specifically concerning supervised learning we see two main categories which are classification or regression. Classification models as stated above work by assigning test data in different categories, or categories spam (as we did our our midterm). Regression analysis is used to see if the independent variables of choice have a relationship with the dependent variable of choice. On the other hand, unsupervised learning is used to analyze and cluster unlabeled data sets, therfore we use this method to find patterns that we could not have seen ourselves. We have used various methods when it comes to unsupervised learning such as clustering, association, and dimensionality. In conclusion, we can see that the mainn difference betweeen these two approaches is based on the type of data that we are given. If we have labeled data, one would use supervised leanring methods and if we do not have labeled data and we re looking for patterns to some sort we wil use unsupervised learning methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3ue1xA46QVyk"
   },
   "source": [
    "# 3. Is supervised or unsupervised learning the primary approach that is used by machine learning practitioners?  For whatever approach you think is secondary, why would you use this approach (what's a good reason to use these kinds of models?)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U3juWLOoQVyk"
   },
   "source": [
    "I would say that the primary approach used by machine learning practitioners is supervised learning which uses labeled data. This is because this approach is used to make predictions and learns efficiently... the learning process ends when the moel has reached a good level of performance. Of course the two types of models that can be used in this approach is classification and regression. On the flip side, unsupervised machine learning can be used as well for unlabeled data where we are seeking to cluster data for example. It all depends on the data that is given in order to determine what the best approach is, for exmaple there can be instances where both approaches have to be used together!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3McC_qocQVyk"
   },
   "source": [
    "# 4. Which unsupervised learning modeling approaches did we cover this semester?  What are the major differences between these techniques?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Sx-418i8QVyl"
   },
   "source": [
    "Unsupervised learning is used to analyze and cluster unlabeled data sets, therfore we use this method to find patterns that we could not have seen ourselves. We have used various methods when it comes to unsupervised learning such as clustering, association, and dimensionality. Some techniques we have learned include k-means clustering, KNN, hierarchal clustering, neaural networks, and PCA. For techniques associated with clustering we are looking to group data together based on similairty, for association methods we are seeking to find relationships between items in the dataset that is given. As always the differnces within these techniques make these methods unique and it is up to the researcher to make the decison as to why and how one method would work better than another based on the data and what they are trying to discover. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cYyDaTmnQVym"
   },
   "source": [
    "# 5.  What are the main benefits of using Principal Components Analysis?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qECJgwwZQVym"
   },
   "source": [
    "Principal Components Analysis (PCA) is a technique used within unsupervised learning. Some benefits that come with using this method is being able to decrease the dimensionality of the data that is given. At times we can get data that runs slow due to high dimensionality. But using PCA method, it can help generate the model and improve the performance of the model withtin harming the accuracy much. In addition, PCA helps reduce noice in data as well as being able to show uncorrelated features within the data that is being used. Lastly, we can always use PCA to visualize data and have that option for clustering. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J3Mz_zKCQVym"
   },
   "source": [
    "# 6. Thinking about neural networks, what are three major differences between a deep multilayer perceptron network and a convolutional neural network model?  Be sure to define any key terms in your explanation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pm9jQfkFQVym"
   },
   "source": [
    "The main differneces between deep multilayer perception network and convolutional neaural network model is that convolutional models are mainly used for image data. We have seen that this feature uses less parameters and attempts to lessen the dimensions of image. It also uses specila convolution and pooling layers. Finally this type of model is complex in nature and is typically used for big data. On the other hand multilayer network is mainly used for more structural data and the parameters is based on the type of data that is being used. Multilayer perception is more simpler compared to convolutional models. It isnt typically used for big data because it is a network of neurons (where in convolutional we use features such as pooling layers for exmaple). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "59xNnlL2QVyn"
   },
   "source": [
    "# 7. Write the tf.keras code for a multilayer perceptron neural network with the following structure: Three hidden layers.  50 hidden units in the first hidden layer, 100 in the second, and 150 in the third.  Activate all hidden layers with relu.  The output layer should be built to classify to five categories.  Further, your optimization technique should be stochastic gradient descent.  (This code should simply build the architecture of the model and your approach to compile the model.  You will not run it on real data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "sowc5PJoSAlu"
   },
   "outputs": [],
   "source": [
    "model17= Sequential ()\n",
    "model17.add(Dense(50,activation='relu',input_dim=69))\n",
    "model17.add(Dense(100,activation='relu'))\n",
    "model17.add(Dense(150,activation='relu'))\n",
    "model17.add(Dense(5,activation='softmax'))\n",
    "sgd =SGD(learning_rate=0.01)\n",
    "model17.compile(optimizer=sgd,\n",
    "               loss='categorical_crossentropy',\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u_rUjo67QVyn"
   },
   "source": [
    "# 8. Write the tf.keras code for a multilayer perceptron neural network with the following structure: Two hidden layers.  75 hidden units in the first hidden layer and 150 in the second.  Activate all hidden layers with relu.  The output layer should be built to classify a binary dependent variable.  Further, your optimization technique should be stochastic gradient descent. (This code should simply build the architecture of the model and your approach to compile the model.  You will not run it on real data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "id": "HwjZFAw6QVyo"
   },
   "outputs": [],
   "source": [
    "model18= Sequential ()\n",
    "model18.add(Dense(75,activation='relu',input_dim=69))\n",
    "model18.add(Dense(150,activation='relu'))\n",
    "model18.add(Dense(1,activation='relu'))\n",
    "model18.add(Dense(1,activation='sigmoid'))\n",
    "sgd =SGD(learning_rate=0.01)\n",
    "model18.compile(optimizer=sgd,\n",
    "               loss='binary_crossentropy',\n",
    "               metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hsaZWFvoQVyo"
   },
   "source": [
    "# 9.  Write the tf.keras code for a convolutional neural network with the following structure: Two convolutional layers.  16 filters in the first layer and 28 in the second.  Activate all convolutional layers with relu.  Use max pooling after each convolutional layer with a 2 by 2 filter.  The output layer should be built to classify to ten categories.  Further, your optimization technique should be stochastic gradient descent.  (This code should simply build the architecture of the model and your approach to compile the model.  You will not run it on real data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "jpyd_mx4QVyo"
   },
   "outputs": [],
   "source": [
    "img_rows, img_cols=28,28\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "  shape_ord=(1, img_rows,img_cols)\n",
    "else:\n",
    "    shape_ord= (img_rows, img_cols,1)\n",
    "\n",
    "nb_epoch =5\n",
    "batch_size =2\n",
    "nb_filters1 =16\n",
    "nb_filters2 =28\n",
    "nb_pool =2\n",
    "nb_conv=2\n",
    "nb_classes =10\n",
    "\n",
    "model19=Sequential ()\n",
    "model19.add(Conv2D(nb_filters1, (nb_conv, nb_conv),\n",
    "                   padding='valid', input_shape=shape_ord))\n",
    "model19.add(Activation('relu'))\n",
    "model19.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\n",
    "\n",
    "model19.add(Conv2D(nb_filters2, (nb_conv, nb_conv)))\n",
    "model19.add(Activation('relu'))\n",
    "model19.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\n",
    "\n",
    "model19.add(Flatten())\n",
    "model19.add(Dense(nb_classes))\n",
    "model19.add(Activation('softmax'))\n",
    "\n",
    "model19.compile(loss='categorical_crossentropy',\n",
    "                optimizer='sgd',\n",
    "                metrics=['accuracy']) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XevgsVAgQVyo"
   },
   "source": [
    "# 10.  Write the keras code for a convolutional neural network with the following structure: Two convolutional layers.  32 filters in the first layer and 32 in the second.  Activate all convolutional layers with relu.  Use max pooling after each convolutional layer with a 2 by 2 filter.  Add two fully connected layers with 128 hidden units in each layer and relu activations.  The output layer should be built to classify to six categories.  Further, your optimization technique should be stochastic gradient descent.  (This code should simply build the architecture of the model and your approach to compile the model.  You will not run it on real data.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "FJnrFmcUQVyo"
   },
   "outputs": [],
   "source": [
    "img_rows, img_cols=28,28\n",
    "\n",
    "if K.image_data_format() == 'channels_first':\n",
    "  shape_ord=(1, img_rows,img_cols)\n",
    "else:\n",
    "    shape_ord= (img_rows, img_cols,1)\n",
    "\n",
    "nb_epoch =5\n",
    "batch_size =2\n",
    "nb_filters1 =32\n",
    "nb_filters2 =32\n",
    "nb_pool =2\n",
    "nb_conv=2\n",
    "nb_classes =6\n",
    "\n",
    "model10=Sequential ()\n",
    "model10.add(Conv2D(nb_filters1, (nb_conv, nb_conv),\n",
    "                   padding='valid', input_shape=shape_ord))\n",
    "model10.add(Activation('relu'))\n",
    "model10.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\n",
    "\n",
    "model10.add(Conv2D(nb_filters2, (nb_conv, nb_conv)))\n",
    "model10.add(Activation('relu'))\n",
    "model10.add(MaxPooling2D(pool_size=(nb_pool, nb_pool)))\n",
    "\n",
    "model10.add(Flatten())\n",
    "model10.add(Dense(128))\n",
    "model10.add(Activation('relu'))\n",
    "model10.add(Activation(('relu')))\n",
    "model10.add(Dense(nb_classes))\n",
    "model10.add(Activation('softmax'))\n",
    "\n",
    "model10.compile(loss='categorical_crossentropy',\n",
    "                optimizer='sgd',\n",
    "                metrics=['accuracy']) "
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "ML FINAL EXAM.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
